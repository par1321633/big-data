{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# import os\n",
    "# spark_location='/opt/spark-2.4.3/' # Set your own\n",
    "# java8_location= '/usr/lib/jvm/java-8-openjdk-amd64' # Set your own\n",
    "# os.environ['JAVA_HOME'] = java8_location\n",
    "# findspark.init(spark_home=spark_location) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\A.Ram\\\\Desktop\\\\BI_Work\\\\parkash_work'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A.Ram\\spark\\spark-2.4.3-bin-hadoop2.7\n",
      "C:\\Program Files\\Java\\jdk1.8.0_221\n",
      "C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files\\Intel\\WiFi\\bin\\;C:\\Program Files\\Common Files\\Intel\\WirelessCommon\\;C:\\Program Files (x86)\\Common Files\\Hitachi ID\\;C:\\Program Files\\Common Files\\Hitachi ID\\;C:\\Users\\.ARam\\AppData\\Local\\pip;C:\\Users\\A.Ram\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts;C:\\Program Files\\TortoiseGit\\bin;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\130\\Tools\\Binn\\;C:\\Program Files (x86)\\Microsoft SQL Server\\140\\Tools\\Binn\\;C:\\Program Files\\Microsoft SQL Server\\140\\Tools\\Binn\\;C:\\Program Files\\Microsoft SQL Server\\140\\DTS\\Binn\\;C:\\Program Files (x86)\\Microsoft SQL Server\\150\\DTS\\Binn\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Users\\A.Ram\\AppData\\Local\\Continuum\\anaconda3;C:\\Users\\A.Ram\\AppData\\Local\\Continuum\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\A.Ram\\AppData\\Local\\Continuum\\anaconda3\\Library\\usr\\bin;C:\\Users\\A.Ram\\AppData\\Local\\Continuum\\anaconda3\\Library\\bin;C:\\Users\\A.Ram\\AppData\\Local\\Continuum\\anaconda3\\Scripts;C:\\Users\\A.Ram\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts;C:\\Users\\A.Ram\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\A.Ram\\AppData\\Local\\Programs\\Git\\cmd;C:\\Program Files\\Java\\jdk1.8.0_221\\bin;C:\\Users\\A.Ram\\spark\\spark-2.4.3-bin-hadoop2.7\\hadoop\\bin;;C:\\Users\\A.Ram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['PATH']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"../2019*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[MATERIAL: string, PLNT: string, BATCH: string, UNRESTRICTED: string, TRANSIT/TRANSF.: string, IN QUALITY INSP.: string, RESTRICTED-USE: string, BLOCKED: string, RETURNS: string, STATUS_DATE: string]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MATERIAL',\n",
       " 'PLNT',\n",
       " 'BATCH',\n",
       " 'UNRESTRICTED',\n",
       " 'TRANSIT/TRANSF.',\n",
       " 'IN QUALITY INSP.',\n",
       " 'RESTRICTED-USE',\n",
       " 'BLOCKED',\n",
       " 'RETURNS',\n",
       " 'STATUS_DATE']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('UNRESTRICTED',\"st1\").withColumnRenamed(\"TRANSIT/TRANSF.\",\"st2\").withColumnRenamed(\"MATERIAL\",'sku_code').withColumnRenamed(\"PLNT\",\"sc_node_code\").withColumnRenamed(\"STATUS_DATE\",\"status_date\").withColumnRenamed(\"BATCH\",\"batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('IN QUALITY INSP.',\"IN_QUALITY_INSP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sku_code: string, sc_node_code: string, batch: string, st1: string, st2: string, IN_QUALITY_INSP: string, RESTRICTED-USE: string, BLOCKED: string, RETURNS: string, status_date: string]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sku_code',\n",
       " 'sc_node_code',\n",
       " 'batch',\n",
       " 'st1',\n",
       " 'st2',\n",
       " 'IN_QUALITY_INSP',\n",
       " 'RESTRICTED-USE',\n",
       " 'BLOCKED',\n",
       " 'RETURNS',\n",
       " 'status_date']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('st3', df['IN_QUALITY_INSP'] + df['RESTRICTED-USE'] + df['BLOCKED'] + df['RETURNS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['IN_QUALITY_INSP','RESTRICTED-USE','BLOCKED','RETURNS']\n",
    "df = df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sku_code: string, sc_node_code: string, batch: string, st1: string, st2: string, status_date: string, st3: double]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"st1\", df[\"st1\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"st2\", df[\"st2\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sku_code', 'string'),\n",
       " ('sc_node_code', 'string'),\n",
       " ('batch', 'string'),\n",
       " ('st1', 'double'),\n",
       " ('st2', 'double'),\n",
       " ('status_date', 'string'),\n",
       " ('st3', 'double')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring, length, col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "?substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+----------+------+----+-----------+---+------------+-----------+\n",
      "|     sku_code|sc_node_code|     batch|   st1| st2|status_date|st3|length_check|batch_check|\n",
      "+-------------+------------+----------+------+----+-----------+---+------------+-----------+\n",
      "|RL10J0AS10AH1|        1002|LIAW      |2136.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        AP01|LIAW      |   4.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        AP02|LIAW      |   2.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        AP05|LIAW      |   2.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        AP06|LIAW      |   2.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        AP10|LIAW      |   2.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        AP12|LIAW      |   2.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        AS01|LIAW      |  30.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        AS03|LIAW      |   1.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        ASR1|LIAW      |  10.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        CGR1|          |   0.0|50.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        DL02|LIAW      |   2.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        GA01|LIAW      |   5.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        GJ01|LIAW      |   5.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        GJ02|LIAW      |   5.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        GJ03|LIAW      |   4.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        GJ04|LIAW      |   2.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        GJ06|LIAW      |   4.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        GJ07|LIAW      |   2.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "|RL10J0AS10AH1|        GJ08|LIAW      |   2.0| 0.0| 2019-06-22|0.0|          13|           |\n",
      "+-------------+------------+----------+------+----+-----------+---+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring, length, col, expr\n",
    "df=df.withColumn('length_check', length('sku_code'))\n",
    "# df.withColumn('b', df.batch.substring(1, 3))\n",
    "df=df.withColumn(\"batch_check\", substring(df.batch,-1,2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df = df.withColumn(\n",
    "    'sales_channel_code',\n",
    "    F.when((F.col(\"batch_check\") == 'AW'),'REP')\\\n",
    "    .when((F.col(\"batch_check\") == 'KZ'),'REP')\\\n",
    "    .when((F.col(\"batch_check\") == 'CC'),'REP')\\\n",
    "    .when((F.col(\"batch_check\") == 'DF'),'OEM')\\\n",
    "    .when((F.col(\"batch_check\") == 'ST'),'OEM')\\\n",
    "    .when((F.col(\"batch_check\") == 'OE'),'OEM')\\\n",
    "    .when((F.col(\"batch_check\") == 'EX'),'EX')\\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=('sales_channel_code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"status_date\",\"sku_code\",\"sc_node_code\",\"sales_channel_code\",\n",
    "              \"stack(3, 'st1', st1, 'st2', st2, 'st3' ,st3) as (stock_type, stock_qty)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[status_date: string, sku_code: string, sc_node_code: string, sales_channel_code: string, stock_type: string, stock_qty: double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+------------------+----------+---------+\n",
      "|status_date|     sku_code|sc_node_code|sales_channel_code|stock_type|stock_qty|\n",
      "+-----------+-------------+------------+------------------+----------+---------+\n",
      "| 2019-06-24|RL10J0AS10AH1|        1002|               REP|       st1|     80.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        1002|               REP|       st2|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        1002|               REP|       st3|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        1002|               REP|       st1|   2036.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        1002|               REP|       st2|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        1002|               REP|       st3|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP01|               REP|       st1|      3.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP01|               REP|       st2|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP01|               REP|       st3|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP02|               REP|       st1|      2.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP02|               REP|       st2|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP02|               REP|       st3|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP05|               REP|       st1|     17.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP05|               REP|       st2|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP05|               REP|       st3|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP06|               REP|       st1|      2.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP06|               REP|       st2|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP06|               REP|       st3|      0.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP10|               REP|       st1|      2.0|\n",
      "| 2019-06-24|RL10J0AS10AH1|        AP10|               REP|       st2|      0.0|\n",
      "+-----------+-------------+------------+------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"../forecast_june.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------+-------+-------------+------------+------------------+------------+----------------+\n",
      "|source_tag|for_month|for_year|in_month|in_year|     sku_code|sc_node_code|sales_channel_code|forecast_qty|  forecast_value|\n",
      "+----------+---------+--------+--------+-------+-------------+------------+------------------+------------+----------------+\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        1002|               REP|   1946.0000|4378500.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        AP01|               REP|    300.0000| 675000.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        AP02|               REP|     15.0000|  33750.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        AP03|               REP|     30.0000|  67500.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        AP04|               REP|    200.0000| 450000.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        AP05|               REP|     30.0000|  67500.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        AP06|               REP|     20.0000|  45000.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        AP07|               REP|     30.0000|  67500.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        AP10|               REP|     30.0000|  67500.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        AP12|               REP|     10.0000|  22500.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        BR01|               REP|     20.0000|  45000.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        BR02|               REP|     10.0000|  22500.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        BR03|               REP|      0.0000|      0.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        BR04|               REP|      4.0000|   9000.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        BR05|               REP|     20.0000|  45000.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        BRR2|               REP|     54.0000| 121500.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        CG01|               REP|    100.0000| 225000.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        CG02|               REP|      0.0000|      0.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        CG03|               REP|     10.0000|  22500.00000000|\n",
      "|         1|        6|    2019|       6|   2019|RL10J0AS10AH1|        CG05|               REP|      2.0000|   4500.00000000|\n",
      "+----------+---------+--------+--------+-------+-------------+------------+------------------+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forecast_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"../calendar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df = calendar_df[['for_date','for_month','for_year','days_in_month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('for_date', 'string'),\n",
       " ('for_month', 'string'),\n",
       " ('for_year', 'string'),\n",
       " ('days_in_month', 'string')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-------------+\n",
      "|  for_date|for_month|for_year|days_in_month|\n",
      "+----------+---------+--------+-------------+\n",
      "|29-01-2013|        1|    2013|           31|\n",
      "|25-01-2013|        1|    2013|           31|\n",
      "|28-01-2013|        1|    2013|           31|\n",
      "|27-01-2013|        1|    2013|           31|\n",
      "|26-01-2013|        1|    2013|           31|\n",
      "|30-01-2013|        1|    2013|           31|\n",
      "|31-01-2013|        1|    2013|           31|\n",
      "|08-02-2013|        2|    2013|           28|\n",
      "|13-02-2013|        2|    2013|           28|\n",
      "|28-02-2013|        2|    2013|           28|\n",
      "|27-02-2013|        2|    2013|           28|\n",
      "|26-04-2013|        4|    2013|           30|\n",
      "|25-04-2013|        4|    2013|           30|\n",
      "|21-06-2013|        6|    2013|           30|\n",
      "|29-07-2013|        7|    2013|           31|\n",
      "|26-07-2013|        7|    2013|           31|\n",
      "|09-02-2013|        2|    2013|           28|\n",
      "|11-02-2013|        2|    2013|           28|\n",
      "|12-02-2013|        2|    2013|           28|\n",
      "|10-02-2013|        2|    2013|           28|\n",
      "+----------+---------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calendar_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date\n",
    "calendar_df = calendar_df.withColumn('for_date', \n",
    "                   to_date(unix_timestamp(col('for_date'), 'dd-MM-yyyy').cast(\"timestamp\")))\n",
    "# df.show()\n",
    "# df.printSchema()\n",
    "# # calendar_df = calendar_df.withColumn(\"for_date\", calendar_df[\"for_date\"].cast(DateType()))\n",
    "# # val df = sc.parallelize(Seq(\"2016-08-26\")).toDF(\"Id\")\n",
    "# # val df2 = df.withColumn(\"Timestamp\", (col(\"Id\").cast(\"timestamp\")))\n",
    "# calendar_df = calendar_df.withColumn(\"Date\", (calendar_df[\"for_date\"].cast(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-------------+\n",
      "|  for_date|for_month|for_year|days_in_month|\n",
      "+----------+---------+--------+-------------+\n",
      "|2013-01-29|        1|    2013|           31|\n",
      "|2013-01-25|        1|    2013|           31|\n",
      "|2013-01-28|        1|    2013|           31|\n",
      "|2013-01-27|        1|    2013|           31|\n",
      "|2013-01-26|        1|    2013|           31|\n",
      "|2013-01-30|        1|    2013|           31|\n",
      "|2013-01-31|        1|    2013|           31|\n",
      "|2013-02-08|        2|    2013|           28|\n",
      "|2013-02-13|        2|    2013|           28|\n",
      "|2013-02-28|        2|    2013|           28|\n",
      "|2013-02-27|        2|    2013|           28|\n",
      "|2013-04-26|        4|    2013|           30|\n",
      "|2013-04-25|        4|    2013|           30|\n",
      "|2013-06-21|        6|    2013|           30|\n",
      "|2013-07-29|        7|    2013|           31|\n",
      "|2013-07-26|        7|    2013|           31|\n",
      "|2013-02-09|        2|    2013|           28|\n",
      "|2013-02-11|        2|    2013|           28|\n",
      "|2013-02-12|        2|    2013|           28|\n",
      "|2013-02-10|        2|    2013|           28|\n",
      "+----------+---------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calendar_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, lit\n",
    "from datetime import datetime\n",
    "date_start = '2019-06-01'\n",
    "date_end ='2019-06-30'\n",
    "date_start = datetime.strptime(str(date_start),'%Y-%m-%d')\n",
    "date_end = datetime.strptime(str(date_end),'%Y-%m-%d')\n",
    "\n",
    "calendar_df=calendar_df.filter(calendar_df[\"for_date\"]>=date_start).filter(calendar_df[\"for_date\"]<=date_end)\n",
    "\n",
    "# .filter(calendar_df[\"for_date\"].lt(lit(date_end)))     \n",
    "\n",
    "# dates = (\"2019-06-01\",  \"2019-06-30\")\n",
    "# from pyspark.sql.functions import col\n",
    "# calendar_df.where(col('for_date').between(*dates)).show(truncate=False)\n",
    "\n",
    "# calendar_df=calendar_df.withColumn(\"for_date\", to_date(unix_timestamp(calendar_df[\"for_date\"], \"MM/dd/yyyy\").cast(\"timestamp\")))\n",
    "# calendar_df.filter(calendar_df[\"for_date\"] >= lit('2017-01-01')) \\\n",
    "#        .filter(calendar_df[\"for_date\"] <= lit('2017-01-31')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-------------+\n",
      "|  for_date|for_month|for_year|days_in_month|\n",
      "+----------+---------+--------+-------------+\n",
      "|2019-06-02|        6|    2019|           30|\n",
      "|2019-06-04|        6|    2019|           30|\n",
      "|2019-06-03|        6|    2019|           30|\n",
      "|2019-06-01|        6|    2019|           30|\n",
      "|2019-06-07|        6|    2019|           30|\n",
      "|2019-06-05|        6|    2019|           30|\n",
      "|2019-06-06|        6|    2019|           30|\n",
      "|2019-06-09|        6|    2019|           30|\n",
      "|2019-06-08|        6|    2019|           30|\n",
      "|2019-06-10|        6|    2019|           30|\n",
      "|2019-06-17|        6|    2019|           30|\n",
      "|2019-06-30|        6|    2019|           30|\n",
      "|2019-06-24|        6|    2019|           30|\n",
      "|2019-06-29|        6|    2019|           30|\n",
      "|2019-06-25|        6|    2019|           30|\n",
      "|2019-06-23|        6|    2019|           30|\n",
      "|2019-06-27|        6|    2019|           30|\n",
      "|2019-06-26|        6|    2019|           30|\n",
      "|2019-06-28|        6|    2019|           30|\n",
      "|2019-06-13|        6|    2019|           30|\n",
      "+----------+---------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calendar_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[for_date: date, for_month: string, for_year: string, days_in_month: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = forecast_df[['sku_code','sc_node_code','sales_channel_code','for_month','for_year']]\n",
    "products_df=products_df.dropDuplicates()\n",
    "# products_df = forecast_df.dropDuplicates(['sku_code','sc_node_code','sales_channel_code','for_month','for_year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+------------------+---------+--------+\n",
      "|     sku_code|sc_node_code|sales_channel_code|for_month|for_year|\n",
      "+-------------+------------+------------------+---------+--------+\n",
      "|RL1AK0AH10AW1|        GJ09|               REP|        6|    2019|\n",
      "|RL1AL0AH10AW1|        GJ02|               REP|        6|    2019|\n",
      "|RLBJKHEMH3AM1|        KL02|               REP|        6|    2019|\n",
      "|RLBJKHEMH3AM1|        MA03|               REP|        6|    2019|\n",
      "|RLBJKHERD1AL2|        AP10|               REP|        6|    2019|\n",
      "|RLBJKHERD1AL2|        GJ03|               REP|        6|    2019|\n",
      "|RLBJKHETA3AL1|        JK01|               REP|        6|    2019|\n",
      "|RLBJKHETA3AL1|        MH03|               REP|        6|    2019|\n",
      "|RLBJKHETA3AL1|        TN01|               REP|        6|    2019|\n",
      "|RLBJLHECC3AM1|        PB03|               REP|        6|    2019|\n",
      "|RLBJLHER21AM1|        CGR1|               REP|        6|    2019|\n",
      "|RLC3QEA623A01|        HR02|               REP|        6|    2019|\n",
      "|RLC3REAR64A01|        OR04|               REP|        6|    2019|\n",
      "|RLC3RFA623A01|        GJ10|               REP|        6|    2019|\n",
      "|RLC3RFA623A01|        RJ03|               REP|        6|    2019|\n",
      "|RLC3RFA623A01|        WB02|               REP|        6|    2019|\n",
      "|RLGAC04GL3AT1|        GJ08|               REP|        6|    2019|\n",
      "|RLGAC0AXL4AT1|        CG05|               REP|        6|    2019|\n",
      "|RLGAE0A3G3AS1|        MP05|               REP|        6|    2019|\n",
      "|RLGAF0A3G3AS1|        JR02|               REP|        6|    2019|\n",
      "+-------------+------------+------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products_df = products_df.dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = calendar_df.join(products_df,\n",
    "         on=['for_month', 'for_year'],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+-------------+-------------+------------+------------------+\n",
      "|for_month|for_year|  for_date|days_in_month|     sku_code|sc_node_code|sales_channel_code|\n",
      "+---------+--------+----------+-------------+-------------+------------+------------------+\n",
      "|        6|    2019|2019-06-02|           30|RYQHJ0TUB0AE1|        DL04|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYN5D0TUB0AE1|        DL01|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYN4Q0TUB0AP1|        PY01|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYN4Q0TUB0AP1|        HRR2|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYN4K0TUB0AP1|        PB01|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYN4G0TFT0AE1|        PB03|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYN0P0TUB0AD1|        MH07|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYN0P0TUB0AD1|        HRR2|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYM7H0MNG0AU1|        WBR2|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYL4F0TUB0AN1|        OR02|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYL4E0TUB0AN1|        HP02|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYJHM0TUB0AD1|        RJ02|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYJDN0TUB0AD1|        RJR4|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYJDN0TUB0AD1|        KA03|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYH6J0TUB0AD1|        KA04|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYH6J0TUB0AD1|        BR03|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYH2D0TUB0AD1|        MH01|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYH2D0TUB0AD1|        CG07|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYH1X0TUB0AD1|        MP02|               REP|\n",
      "|        6|    2019|2019-06-02|           30|RYH1Q0TUB0AD1|        MP06|               REP|\n",
      "+---------+--------+----------+-------------+-------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = products_df.join(forecast_df,\n",
    "         on=['for_month', 'for_year', 'sku_code', 'sc_node_code','sales_channel_code'],\n",
    "         how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = products_df.withColumn('forecast_dly', products_df['forecast_qty']/products_df['days_in_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------+------------+------------------+----------+-------------+----------+--------+-------+------------+--------------+------------+\n",
      "|for_month|for_year|     sku_code|sc_node_code|sales_channel_code|  for_date|days_in_month|source_tag|in_month|in_year|forecast_qty|forecast_value|forecast_dly|\n",
      "+---------+--------+-------------+------------+------------------+----------+-------------+----------+--------+-------+------------+--------------+------------+\n",
      "|        6|    2019|RYQHJ0TUB0AE1|        DL04|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYN5D0TUB0AE1|        DL01|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYN4Q0TUB0AP1|        PY01|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYN4Q0TUB0AP1|        HRR2|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYN4K0TUB0AP1|        PB01|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYN4G0TFT0AE1|        PB03|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYN0P0TUB0AD1|        MH07|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYN0P0TUB0AD1|        HRR2|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYM7H0MNG0AU1|        WBR2|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYL4F0TUB0AN1|        OR02|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYL4E0TUB0AN1|        HP02|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYJHM0TUB0AD1|        RJ02|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYJDN0TUB0AD1|        RJR4|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYJDN0TUB0AD1|        KA03|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYH6J0TUB0AD1|        KA04|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYH6J0TUB0AD1|        BR03|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYH2D0TUB0AD1|        MH01|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYH2D0TUB0AD1|        CG07|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYH1X0TUB0AD1|        MP02|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "|        6|    2019|RYH1Q0TUB0AD1|        MP06|               REP|2019-06-02|           30|         1|       6|   2019|      0.0000|    0.00000000|         0.0|\n",
      "+---------+--------+-------------+------------+------------------+----------+-------------+----------+--------+-------+------------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbDetails = {\n",
    "#     \"driver\": \"postgresql\",\n",
    "#     \"username\": \"postgres\",\n",
    "#     \"password\": \"password\",\n",
    "#     \"host\": \"localhost\",\n",
    "#     \"port\": \"5432\",\n",
    "#     \"database\": \"apollo_alpha\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calendar_df = sqlContext.read \\\n",
    "#     .format(\"odbc\") \\\n",
    "#     .option(\"url\",'jdbc:postgresql://localhost:5432/alpha_emami') \\\n",
    "#     .option(\"dbtable\", \"bi.calendar_master\") \\\n",
    "#     .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "calendar_df = sqlContext.read \\\n",
    "    .format(\"odbc\") \\\n",
    "    .option(\"url\",'jdbc:postgresql://localhost:5432/alpha_emami') \\\n",
    "    .option(\"dbtable\", \"bi.calendar_master\") \\\n",
    "    .load()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('status_date',\"for_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------+------------------+----------+---------+\n",
      "|  for_date|     sku_code|sc_node_code|sales_channel_code|stock_type|stock_qty|\n",
      "+----------+-------------+------------+------------------+----------+---------+\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st1|     80.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st1|   2036.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP01|               REP|       st1|      3.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP01|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP01|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP02|               REP|       st1|      2.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP02|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP02|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP05|               REP|       st1|     17.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP05|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP05|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP06|               REP|       st1|      2.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP06|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP06|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP10|               REP|       st1|      2.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP10|               REP|       st2|      0.0|\n",
      "+----------+-------------+------------+------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, to_date\n",
    "df = df.withColumn('for_date', \n",
    "                   to_date(unix_timestamp(col('for_date'), 'yyyy-MM-dd').cast(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------+------------------+----------+---------+\n",
      "|  for_date|     sku_code|sc_node_code|sales_channel_code|stock_type|stock_qty|\n",
      "+----------+-------------+------------+------------------+----------+---------+\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st1|     80.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st1|   2036.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        1002|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP01|               REP|       st1|      3.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP01|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP01|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP02|               REP|       st1|      2.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP02|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP02|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP05|               REP|       st1|     17.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP05|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP05|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP06|               REP|       st1|      2.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP06|               REP|       st2|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP06|               REP|       st3|      0.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP10|               REP|       st1|      2.0|\n",
      "|2019-06-24|RL10J0AS10AH1|        AP10|               REP|       st2|      0.0|\n",
      "+----------+-------------+------------+------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = products_df.join(df,\n",
    "                 on=['sku_code', 'sc_node_code','sales_channel_code','for_date'],\n",
    "                 how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_df.write.format('com.databricks.spark.csv').save('../final_stock_secondary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# final_df.write.format('csv').save('../final_stock_secondary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_df.write.parquet(\"../proto.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+------------------+----------+---------+--------+-------------+----------+--------+-------+------------+---------------+-------------------+----------+---------+\n",
      "|     sku_code|sc_node_code|sales_channel_code|  for_date|for_month|for_year|days_in_month|source_tag|in_month|in_year|forecast_qty| forecast_value|       forecast_dly|stock_type|stock_qty|\n",
      "+-------------+------------+------------------+----------+---------+--------+-------------+----------+--------+-------+------------+---------------+-------------------+----------+---------+\n",
      "|RL10J0AS10AH1|        AP12|               REP|2019-06-16|        6|    2019|           30|         1|       6|   2019|     10.0000| 22500.00000000| 0.3333333333333333|      null|      0.0|\n",
      "|RL10J0AS10AH1|        BR03|               REP|2019-06-02|        6|    2019|           30|         1|       6|   2019|      0.0000|     0.00000000|                0.0|      null|      0.0|\n",
      "|RL10J0AS10AH1|        BR04|               REP|2019-06-08|        6|    2019|           30|         1|       6|   2019|      4.0000|  9000.00000000|0.13333333333333333|      null|      0.0|\n",
      "|RL10J0AS10AH1|        CG01|               REP|2019-06-26|        6|    2019|           30|         1|       6|   2019|    100.0000|225000.00000000| 3.3333333333333335|      null|      0.0|\n",
      "|RL10J0AS10AH1|        DL01|               REP|2019-06-07|        6|    2019|           30|         1|       6|   2019|      5.0000| 11250.00000000|0.16666666666666666|      null|      0.0|\n",
      "|RL10J0AS10AH1|        GJ01|               REP|2019-06-17|        6|    2019|           30|         1|       6|   2019|     10.0000| 22500.00000000| 0.3333333333333333|      null|      0.0|\n",
      "|RL10J0AS10AH1|        GJ02|               REP|2019-06-25|        6|    2019|           30|         1|       6|   2019|      8.0000| 18000.00000000|0.26666666666666666|       st1|      5.0|\n",
      "|RL10J0AS10AH1|        GJ02|               REP|2019-06-25|        6|    2019|           30|         1|       6|   2019|      8.0000| 18000.00000000|0.26666666666666666|       st2|      0.0|\n",
      "|RL10J0AS10AH1|        GJ02|               REP|2019-06-25|        6|    2019|           30|         1|       6|   2019|      8.0000| 18000.00000000|0.26666666666666666|       st3|      0.0|\n",
      "|RL10J0AS10AH1|        GJ09|               REP|2019-06-17|        6|    2019|           30|         1|       6|   2019|      4.0000|  9000.00000000|0.13333333333333333|      null|      0.0|\n",
      "|RL10J0AS10AH1|        HR03|               REP|2019-06-29|        6|    2019|           30|         1|       6|   2019|      5.0000| 11250.00000000|0.16666666666666666|      null|      0.0|\n",
      "|RL10J0AS10AH1|        HR08|               REP|2019-06-25|        6|    2019|           30|         1|       6|   2019|      5.0000| 11250.00000000|0.16666666666666666|       st1|     12.0|\n",
      "|RL10J0AS10AH1|        HR08|               REP|2019-06-25|        6|    2019|           30|         1|       6|   2019|      5.0000| 11250.00000000|0.16666666666666666|       st2|      0.0|\n",
      "|RL10J0AS10AH1|        HR08|               REP|2019-06-25|        6|    2019|           30|         1|       6|   2019|      5.0000| 11250.00000000|0.16666666666666666|       st3|      0.0|\n",
      "|RL10J0AS10AH1|        HRR2|               REP|2019-06-15|        6|    2019|           30|         1|       6|   2019|      5.0000| 11250.00000000|0.16666666666666666|       st1|     30.0|\n",
      "|RL10J0AS10AH1|        HRR2|               REP|2019-06-15|        6|    2019|           30|         1|       6|   2019|      5.0000| 11250.00000000|0.16666666666666666|       st2|      0.0|\n",
      "|RL10J0AS10AH1|        HRR2|               REP|2019-06-15|        6|    2019|           30|         1|       6|   2019|      5.0000| 11250.00000000|0.16666666666666666|       st3|      0.0|\n",
      "|RL10J0AS10AH1|        JR02|               REP|2019-06-15|        6|    2019|           30|         1|       6|   2019|     20.0000| 45000.00000000| 0.6666666666666666|       st1|     10.0|\n",
      "|RL10J0AS10AH1|        JR02|               REP|2019-06-15|        6|    2019|           30|         1|       6|   2019|     20.0000| 45000.00000000| 0.6666666666666666|       st2|      0.0|\n",
      "|RL10J0AS10AH1|        JR02|               REP|2019-06-15|        6|    2019|           30|         1|       6|   2019|     20.0000| 45000.00000000| 0.6666666666666666|       st3|      0.0|\n",
      "+-------------+------------+------------------+----------+---------+--------+-------------+----------+--------+-------+------------+---------------+-------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = sqlContext.read.parquet('../proto.parquet')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+---+----------+---+----+---+---+---+----+--------+---------------+-------------------+----+----+\n",
      "|          _c0| _c1|_c2|       _c3|_c4| _c5|_c6|_c7|_c8| _c9|    _c10|           _c11|               _c12|_c13|_c14|\n",
      "+-------------+----+---+----------+---+----+---+---+---+----+--------+---------------+-------------------+----+----+\n",
      "|RL10J0AS10AH1|AP01|REP|2019-06-13|  6|2019| 30|  1|  6|2019|300.0000|675000.00000000|               10.0|null| 0.0|\n",
      "|RL10J0AS10AH1|AP04|REP|2019-06-09|  6|2019| 30|  1|  6|2019|200.0000|450000.00000000|  6.666666666666667|null| 0.0|\n",
      "|RL10J0AS10AH1|AP06|REP|2019-06-03|  6|2019| 30|  1|  6|2019| 20.0000| 45000.00000000| 0.6666666666666666|null| 0.0|\n",
      "|RL10J0AS10AH1|AP07|REP|2019-06-13|  6|2019| 30|  1|  6|2019| 30.0000| 67500.00000000|                1.0|null| 0.0|\n",
      "|RL10J0AS10AH1|CG03|REP|2019-06-25|  6|2019| 30|  1|  6|2019| 10.0000| 22500.00000000| 0.3333333333333333|null| 0.0|\n",
      "|RL10J0AS10AH1|GA01|REP|2019-06-02|  6|2019| 30|  1|  6|2019| 12.0000| 27000.00000000|                0.4|null| 0.0|\n",
      "|RL10J0AS10AH1|GJ03|REP|2019-06-26|  6|2019| 30|  1|  6|2019|  8.0000| 18000.00000000|0.26666666666666666| st1| 1.0|\n",
      "|RL10J0AS10AH1|GJ03|REP|2019-06-26|  6|2019| 30|  1|  6|2019|  8.0000| 18000.00000000|0.26666666666666666| st2| 0.0|\n",
      "|RL10J0AS10AH1|GJ03|REP|2019-06-26|  6|2019| 30|  1|  6|2019|  8.0000| 18000.00000000|0.26666666666666666| st3| 0.0|\n",
      "|RL10J0AS10AH1|GJ05|REP|2019-06-27|  6|2019| 30|  1|  6|2019|  2.0000|  4500.00000000|0.06666666666666667|null| 0.0|\n",
      "|RL10J0AS10AH1|GJ06|REP|2019-06-17|  6|2019| 30|  1|  6|2019|  4.0000|  9000.00000000|0.13333333333333333|null| 0.0|\n",
      "|RL10J0AS10AH1|GJR1|REP|2019-06-07|  6|2019| 30|  1|  6|2019| 62.0000|139500.00000000|  2.066666666666667|null| 0.0|\n",
      "|RL10J0AS10AH1|HR02|REP|2019-06-08|  6|2019| 30|  1|  6|2019|  5.0000| 11250.00000000|0.16666666666666666|null| 0.0|\n",
      "|RL10J0AS10AH1|MH05|REP|2019-06-06|  6|2019| 30|  1|  6|2019| 10.0000| 22500.00000000| 0.3333333333333333|null| 0.0|\n",
      "|RL10J0AS10AH1|MH07|REP|2019-06-22|  6|2019| 30|  1|  6|2019| 10.0000| 22500.00000000| 0.3333333333333333|null| 0.0|\n",
      "|RL10J0AS10AH1|MH08|REP|2019-06-12|  6|2019| 30|  1|  6|2019| 15.0000| 33750.00000000|                0.5|null| 0.0|\n",
      "|RL10J0AS10AH1|MP05|REP|2019-06-24|  6|2019| 30|  1|  6|2019| 20.0000| 45000.00000000| 0.6666666666666666| st1| 4.0|\n",
      "|RL10J0AS10AH1|MP05|REP|2019-06-24|  6|2019| 30|  1|  6|2019| 20.0000| 45000.00000000| 0.6666666666666666| st2| 0.0|\n",
      "|RL10J0AS10AH1|MP05|REP|2019-06-24|  6|2019| 30|  1|  6|2019| 20.0000| 45000.00000000| 0.6666666666666666| st3| 0.0|\n",
      "|RL10J0AS10AH1|RJ07|REP|2019-06-10|  6|2019| 30|  1|  6|2019|  5.0000| 11250.00000000|0.16666666666666666|null| 0.0|\n",
      "+-------------+----+---+----------+---+----+---+---+---+----+--------+---------------+-------------------+----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = sqlContext.read.csv('../final_stock_secondary.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4735661, 15)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spark_shape(self):\n",
    "    return (self.count(), len(self.columns))\n",
    "pyspark.sql.dataframe.DataFrame.shape = spark_shape\n",
    "df.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "-Read and Write to Database\n",
    "-Make the system as master and VM's as slave and run the code\n",
    "-Check is alphabet function and picking last two elements of column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
